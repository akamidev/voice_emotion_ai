import cv2
from deepface import DeepFace
from PIL import Image, ImageTk

def detect_emotion_live_tk(video_label):
    cap = cv2.VideoCapture(0)

    if not cap.isOpened():
        print("âŒ Impossible d'ouvrir la camÃ©ra.")
        return

    print("ğŸ“¸ CamÃ©ra dÃ©marrÃ©e. Appuie sur 'q' pour quitter.")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        try:
            result = DeepFace.analyze(frame, actions=["emotion"], enforce_detection=False)
            emotion = result[0]["dominant_emotion"]

            # Traduction des Ã©motions en franÃ§ais
            emotions_fr = {
                "neutral": "ğŸ˜ Neutre",
                "happy": "ğŸ˜Š Heureux",
                "sad": "ğŸ˜¢ Triste",
                "angry": "ğŸ˜  En colÃ¨re",
                "fear": "ğŸ˜¨ Peur",
                "surprise": "ğŸ˜² Surprise",
                "disgust": "ğŸ¤¢ DÃ©goÃ»t"
            }

            label = f"Ã‰motion : {emotions_fr.get(emotion, emotion)}"
            cv2.putText(frame, label, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        except:
            cv2.putText(frame, "Pas de visage dÃ©tectÃ©", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        # Convertir l'image OpenCV en image Tkinter
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = Image.fromarray(rgb_frame)
        imgtk = ImageTk.PhotoImage(image=img)
        video_label.configure(image=imgtk)
        video_label.image = imgtk

        # Pour Ã©viter un freeze complet de lâ€™UI
        video_label.update_idletasks()

        # Appuie sur Q pour quitter
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    video_label.configure(image="")
    print("ğŸ›‘ CamÃ©ra arrÃªtÃ©e.")
